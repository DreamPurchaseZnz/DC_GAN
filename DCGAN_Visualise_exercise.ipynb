{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense , Activation , Flatten , Reshape \n",
    "# convolutional layers\n",
    "from keras.layers.convolutional import Conv2D , UpSampling2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "# other module\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from time import clock as now\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generator and Discriminator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024,input_dim=100,name='gen_layer1'))\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.add(Dense(128*7*7,name='gen_layer2'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.add(Reshape((128,7,7),input_shape=(128*7*7,)))\n",
    "    model.add(UpSampling2D(size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(filters=64,kernel_size=(5,5),padding='same',name='gen_layer3'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(1,(5,5),padding='same',name='gen_output'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,(5,5),padding='same',input_shape=(1,28,28),name='dcm_layer1'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(128,(5,5),padding='same',name='dcm_layer2'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1024,name='dcm_layer3'))\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.add(Dense(1,name='dcm_output'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Generator_D and Classifier  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_containing_discriminator(generator,discriminator):\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64,(5,5),padding='same',input_shape=(1,28,28),name='dcm_layer1'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(128,(5,5),padding='same',name='dcm_layer2'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1024,name='dcm_layer3'))\n",
    "    model.add(Activation('tanh'))\n",
    "    \n",
    "    model.add(Dense(10,name='clf_output'))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Combine_image and  Generate_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_images(generate_images):\n",
    "    num = generate_images.shape[0]\n",
    "    \n",
    "    shape  = generate_images.shape[2:]\n",
    "    width  = int(math.sqrt(num))\n",
    "    height = math.ceil(num/width)\n",
    "    \n",
    "    image = np.zeros((height*shape[0],width*shape[1]),# shape must be a tuple\n",
    "                     dtype= generate_images.dtype)\n",
    "    for index,img in enumerate(generate_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0],j*shape[1]:(j+1)*shape[1]]=\\\n",
    "            img[0,:,:]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate(BATCH_SIZE,nice=False):\n",
    "    generator = generator_model()\n",
    "    generator.compile(loss = 'binary_crossentropy',optimizer='SGD')\n",
    "    generator.load_weights('gan_generator.h5')\n",
    "    \n",
    "    if nice:\n",
    "        discriminator = discriminator_model()\n",
    "        discriminator.compile(loss='binary_crossentropy',optimizer='SGD')\n",
    "        discriminator.load_weights('gan_discriminator.h5')\n",
    "        \n",
    "        noise = np.zeros((BATCH_SIZE*20,100))\n",
    "        for i in range(BATCH_SIZE*20):\n",
    "            noise[i,:]=np.random.uniform(-1,1,100)\n",
    "        \n",
    "        generated_images = generator.predict(noise,verbose=0)\n",
    "        d_pret = discriminator.predict(generated_images,verbose=0)\n",
    "        \n",
    "        # order\n",
    "        index =  np.arange(0,BATCH_SIZE*20)[:,np.newaxis]\n",
    "        pre_with_index = list(np.append(d_pret,index,axis=1))\n",
    "        pre_with_index.sort(key=lambda x:x[0],reverse=True)\n",
    "        \n",
    "        nice_images = np.zeros((BATCH_SIZE,1)+(generated_images.shape[2:]),dtype=np.float32)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i,0,:,:]=generated_images[idx,0,:,:] # 从零开始计数[idx,1,:,:]会报错out of bounds for axis\n",
    "        image =combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.zeros((BATCH_SIZE,100))\n",
    "        for i in range(BATCH_SIZE):\n",
    "            noise[i,:] = np.random.uniform(-1,1,100)\n",
    "        generated_images = generator.predict(noise,verbose=1)\n",
    "        image =combine_images(nice_images)\n",
    "    \n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save('generated_image.png')                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN_Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(BATCH_SIZE,EPOCH):\n",
    "    (x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "    x_train = (x_train.astype(np.float32)-127.5)/127.5\n",
    "    x_train = x_train.reshape((x_train.shape[0],1,28,28))\n",
    "    \n",
    "    #define models\n",
    "    generator = generator_model()\n",
    "    discriminator = discriminator_model()\n",
    "    discriminator_on_generator = generator_containing_discriminator(generator,discriminator)\n",
    "    \n",
    "    # compile models\n",
    "    d_optim = SGD(lr = 0.0005,momentum =0.9,nesterov=True)\n",
    "    g_optim = SGD(lr = 0.0005,momentum = 0.9,nesterov =True)\n",
    "    generator.compile(loss ='binary_crossentropy',optimizer='SGD')\n",
    "    discriminator_on_generator.compile(loss='binary_crossentropy',optimizer=g_optim)\n",
    "    discriminator.trainable = True\n",
    "    discriminator.compile(loss='binary_crossentropy',optimizer=d_optim)\n",
    "    \n",
    "    # train epoch and bitch size\n",
    "    noise = np.zeros((BATCH_SIZE,100))\n",
    "    for epoch in range(EPOCH):\n",
    "        BATCH_NUM = int(x_train.shape[0]/BATCH_SIZE)\n",
    "        print('---------------------EPOCH IS %d TOTAL OF %d-------------------- '%(epoch,EPOCH))\n",
    "        for index in range(BATCH_NUM):\n",
    "            # create train_batch\n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i,:] = np.random.uniform(-1,1,100)\n",
    "            generated_images = generator.predict(noise,verbose=0)\n",
    "            image_batch = x_train[i*BATCH_SIZE:(i+1)*BATCH_SIZE]\n",
    "        \n",
    "            X = np.concatenate((image_batch,generated_images))\n",
    "            y = [1]*BATCH_SIZE+[0]*BATCH_SIZE\n",
    "            # train the discriminator\n",
    "            d_loss = discriminator.train_on_batch(X,y)\n",
    "            # watch the process\n",
    "            if index%100 == 0 :\n",
    "                print('In batch %d of %d ,dcm loss is:%f'%(index,BATCH_NUM,d_loss))\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save('Process_generator/'+str(epoch)+'_'+str(index)+'.png')\n",
    "            \n",
    "            for i in range(BATCH_SIZE):\n",
    "                noise[i,:]=np.random.uniform(-1,1,100)\n",
    "            discriminator.trainable =False\n",
    "            g_loss = discriminator_on_generator.train_on_batch(noise,[1]*BATCH_SIZE)\n",
    "            discriminator.trainable = True\n",
    "            if index%100 ==0:\n",
    "                print('In batch %d of %d ,gen loss is:%f'%(index,BATCH_NUM,g_loss))\n",
    "        \n",
    "        generator.save_weights('gan_generator.h5')\n",
    "        discriminator.save_weights('gan_discriminator.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_train():\n",
    "    (x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "    x_train = (x_train-127.5)/127.5\n",
    "    x_test  = (x_test -127.5)/127.5\n",
    "    x_train = x_train.reshape([x_train.shape[0],1,28,28])\n",
    "    x_test  = x_test.reshape([x_test.shape[0],1,28,28])\n",
    "    y_train = np_utils.to_categorical(y_train,num_classes = 10)\n",
    "    y_test  = np_utils.to_categorical(y_test, num_classes = 10)\n",
    "    \n",
    "    classifier = classifier_model()\n",
    "    classifier.compile(loss='binary_crossentropy',optimizer='SGD',metrics=['accuracy'])\n",
    "    classifier.load_weights('gan_discriminator.h5',by_name=True)\n",
    "    classifier.fit(x_train,y_train,epochs=10,batch_size=128)\n",
    "    \n",
    "    loss,accuracy=classifier.evaluate(x_test,y_test,verbose = 1)\n",
    "    classifier.save_weights('gan_classifier.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------EPOCH IS 0 TOTAL OF 100-------------------- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2252: UserWarning: Expected no kwargs, you passed 1\n",
      "kwargs passed to function are ignored with Tensorflow backend\n",
      "  warnings.warn('\\n'.join(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In batch 0 of 468 ,dcm loss is:0.663925\n",
      "In batch 0 of 468 ,gen loss is:0.724368\n",
      "In batch 100 of 468 ,dcm loss is:0.210255\n",
      "In batch 100 of 468 ,gen loss is:1.159469\n"
     ]
    }
   ],
   "source": [
    "time1=now()\n",
    "train(BATCH_SIZE=128,EPOCH=100)\n",
    "time2=now()\n",
    "generate(BATCH_SIZE=128,nice=True)\n",
    "classifier_train()\n",
    "print(time2-time1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
